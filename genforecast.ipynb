{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7b26d6cc-c6ec-4389-a8e1-40c45ef8f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0661d6a3-e2ae-43fc-95d8-828b10a46552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse, json\n",
    "from cross_exp.exp_crossformer import Exp_crossformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5890afb-319c-409f-8a0c-8fda1a935ed6",
   "metadata": {},
   "source": [
    "# Define model and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223cec1-0979-4ea1-9548-ffba50655406",
   "metadata": {},
   "source": [
    "Please update the filter for `testdf.index` in `__init__()` of Class, and `fcst` slicing in `getMetrics()` if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "1b8d6026-d3d0-4caa-80d1-169f811e8588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MY_MODEL():\n",
    "    \n",
    "    def __init__(self, testdata):\n",
    "        \"\"\"\n",
    "        Load the trained model and all hyperparameters\n",
    "        \"\"\"\n",
    "        #get mean & std\n",
    "        file = open('submit/scale_statistic.pkl', 'rb') \n",
    "        data = pickle.load(file)\n",
    "      \n",
    "        #stock list to predict\n",
    "        src=pd.read_csv('datasets/styAdj_small_mean.csv')\n",
    "        self.traindf=src\n",
    "        self.stock_pred_list = src.columns[1:]   # load stock_pred_list\n",
    "        src=src.set_index('time')\n",
    "        \n",
    "        self.testdf=self.convert(testdata) ### newparquet\n",
    "        self.testdf=self.testdf.loc[self.testdf.index>=\"2020-12-01\"]### update as necessary\n",
    "        self.testindex=self.testdf.index\n",
    "        #concat train and test\n",
    "        pd.concat([src,self.testdf], axis=0).to_csv('datasets/concat.csv', index=True)\n",
    "        self.fulldf=pd.read_csv('datasets/concat.csv')\n",
    "        \n",
    "        print('loading parameters')\n",
    "        config = json.load(open('submit/args.json'))  # load configuration arguments \n",
    "        config['scale_statistic']=data #get mean and sd\n",
    "        self.args = argparse.Namespace(**config)\n",
    "        \n",
    "        self.args.data_split=[168,len(src)-168,len(self.testdf)]\n",
    "        self.args.data_path=\"concat.csv\"\n",
    "       \n",
    "        print('loading model')\n",
    "        self.my_model = Exp_crossformer(self.args)\n",
    "        self.my_model.model.load_state_dict(torch.load('submit/checkpoint.pth'))   # load trained model\n",
    "       \n",
    "    def convert(self, data):\n",
    "        daily_data = pd.read_parquet(data)\n",
    "        return_type = '1d_next_styAdj'\n",
    "        transformed_1d = daily_data.pivot_table(index='stock_id', columns='time', values=return_type)\n",
    "        test = transformed_1d.T.copy().fillna(0.0)\n",
    "        test.columns = [str(id) for id in test.columns]\n",
    "        test.index.name = 'date'\n",
    "        smallCap = test.loc[:, self.stock_pred_list]\n",
    "        return smallCap\n",
    "\n",
    "    def eval(self):\n",
    "        self.my_model.eval('pred',self.args.save_pred)\n",
    "        mtest=np.load('results/pred/pred.npy')\n",
    "        mdf=pd.DataFrame(mtest.squeeze())\n",
    "        return mdf*self.args.scale_statistic['std']+self.args.scale_statistic['mean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4acb0a4b-45fc-4bb5-9ab4-58eaeb610f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_correlation(X,Y,weight_column):   \n",
    "\n",
    "    # Calculate the weighted means\n",
    "    mean_X = np.sum(X * weight_column) / np.sum(weight_column)\n",
    "    mean_Y = np.sum(Y * weight_column) / np.sum(weight_column)\n",
    "\n",
    "    # Calculate the weighted covariance\n",
    "    weighted_covariance = np.sum(weight_column * (X - mean_X) * (Y - mean_Y))\n",
    "\n",
    "    # Calculate the weighted standard deviations\n",
    "    weighted_std_X = np.sqrt(np.sum(weight_column * (X - mean_X)**2))\n",
    "    weighted_std_Y = np.sqrt(np.sum(weight_column * (Y - mean_Y)**2))\n",
    "\n",
    "    # Calculate the weighted correlation coefficient\n",
    "    weighted_correlation = weighted_covariance / (weighted_std_X * weighted_std_Y)\n",
    "\n",
    "    return weighted_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1d4c9331-1bff-484e-8180-0cdb6255ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(preddf, testindex):\n",
    "    ### Load train and test data\n",
    "    train=pd.read_csv('datasets/styAdj_small_mean.csv')\n",
    "    responseData=pd.read_parquet(testparquet)\n",
    "    print(train.shape)\n",
    "    \n",
    "    if train['time'].dtype != '<M8[ns]':\n",
    "        train['time'] = pd.to_datetime(train['time'])\n",
    "    train = train.set_index('time')\n",
    "\n",
    "    pred=preddf\n",
    "    #test_begin_date = train.tail(pred.shape[0])\n",
    "    pred.columns = train.columns\n",
    "    pred.index = testindex\n",
    "    pred.index.names = ['time']\n",
    "\n",
    "    # 将 DataFrame 从宽格式转换为长格式\n",
    "    long_pred = pred.reset_index().melt(id_vars=['time'], var_name='stock_id', value_name='forecast')\n",
    "    long_pred = long_pred.set_index('time')\n",
    "    \n",
    "    long_pred['stock_id'] = pd.to_numeric(long_pred['stock_id'])\n",
    "    \n",
    "    daily = responseData.copy()\n",
    "    fcst = pd.merge(daily.reset_index(), long_pred.reset_index(), on=['time', 'stock_id'], how='left').fillna(0.0).set_index('time')\n",
    "    fcst=fcst.loc[fcst.index>=\"2020-12-01 10:00:00\"] #new slicing (change this to only predict certain period)\n",
    "    \n",
    "    # IC\n",
    "    weight = fcst['liq_weight'].values\n",
    "    IC = []\n",
    "    for hz in [21, 10, 5, 1]:   \n",
    "        mr = '%sd_next_mktAdj' % hz\n",
    "        target = fcst[mr].values\n",
    "        IC.append(weighted_correlation(fcst['forecast'].values, target, weight))\n",
    "        \n",
    "    y_hat = fcst['forecast'].values\n",
    "    weight = fcst['liq_weight'].values   # always use liq_weight in evaluation\n",
    "\n",
    "    portfolio = fcst[['stock_id','1d_next_mktAdj','liq_weight']].copy()\n",
    "    portfolio['pos'] = weight * y_hat\n",
    "    portfolio['pos'] = portfolio.groupby(portfolio.index)['pos'].transform(lambda x: x/np.sum(np.abs(x)))\n",
    "    pnl = portfolio.groupby(portfolio.index).apply(lambda x: np.sum(x['pos'] * x['1d_next_mktAdj']))\n",
    "    portfolio['prev_pos'] = portfolio.groupby(portfolio.stock_id)['pos'].shift().fillna(0.0)\n",
    "    turnover = portfolio.groupby(portfolio.index).apply(lambda x: np.sum(np.abs(x['pos']-x['prev_pos'])))\n",
    "        \n",
    "    annual_mean = pnl.mean() * 242 * 100\n",
    "    annual_volatility = pnl.std() * np.sqrt(242) * 100\n",
    "    annual_sharpe_ratio = pnl.mean() / pnl.std() * np.sqrt(242)\n",
    "    tr = turnover.mean()\n",
    "    metrics = [annual_mean, annual_volatility, annual_sharpe_ratio, tr]\n",
    "\n",
    "    m1=pd.DataFrame({'Raw forecast':IC}, index=(['IC_21d', 'IC_10d', 'IC_5d','IC_1d']))\n",
    "\n",
    "    m2 = pd.DataFrame({'Raw forecast':metrics}, index=(['Annual Mean (%)', 'Annual Vol(%)', 'Annual Sharpe ratio','Turnover']))\n",
    "    metrics = pd.concat([m1, m2])\n",
    "    return metrics\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ab1b7-4c8c-4686-83e1-023dc6c1c26b",
   "metadata": {},
   "source": [
    "# Generate Forecast and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a717feda-61a1-459a-a8a7-0585158f3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "testparquet='./results/test.parquet' #change it to the unseen test parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "5c38a6f1-5d05-4572-a382-a0cb438fa281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading parameters\n",
      "loading model\n",
      "Use CPU\n",
      "mse:1.350561499595642, mae:0.7241932153701782\n"
     ]
    }
   ],
   "source": [
    "m=MY_MODEL(testparquet)\n",
    "preddf=m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "d5febcbe-b2e4-425d-929a-9dac0b5b4049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2651, 675)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Raw forecast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>IC_21d</th>\n",
       "      <td>0.024763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IC_10d</th>\n",
       "      <td>0.021729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IC_5d</th>\n",
       "      <td>0.019049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IC_1d</th>\n",
       "      <td>0.013278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual Mean (%)</th>\n",
       "      <td>28.308912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual Vol(%)</th>\n",
       "      <td>4.912838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual Sharpe ratio</th>\n",
       "      <td>5.762232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Turnover</th>\n",
       "      <td>0.788196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Raw forecast\n",
       "IC_21d                   0.024763\n",
       "IC_10d                   0.021729\n",
       "IC_5d                    0.019049\n",
       "IC_1d                    0.013278\n",
       "Annual Mean (%)         28.308912\n",
       "Annual Vol(%)            4.912838\n",
       "Annual Sharpe ratio      5.762232\n",
       "Turnover                 0.788196"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getMetrics(preddf, m.testindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fb733-165d-40f6-b6cb-4bc200883cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
